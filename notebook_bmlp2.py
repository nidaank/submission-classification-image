# -*- coding: utf-8 -*-
"""[PCD] Pengenalan Objek_Nida'an Khafiyya.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rzhwvg4R_WLiX7m5p7v3-bsJKy72NbZY

# Proyek Klasifikasi Gambar: Rice Image Dataset
- **Nama:** Nida'an Khafiyya
- **Email:** nidaankhafiyyaakun@gmail.com

## Import Semua Packages/Library yang Digunakan
"""

!pip install tensorflowjs

# Commented out IPython magic to ensure Python compatibility.
# Mengimpor libraries umum yang sering digunakan
import os, shutil
import zipfile
import random
from random import sample
import shutil
from shutil import copyfile
import pathlib
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm as tq

# Mengimpor libraries untuk visualisasi
# %matplotlib inline
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.image import imread

# Mengimpor libraries untuk pemrosesan data gambar
from PIL import Image
from collections import defaultdict

# Mengimpor libraries untuk pembuatan dan evaluasi model
import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.utils.class_weight import compute_class_weight
import tensorflow as tf
from keras import Model, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import InputLayer, Conv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.callbacks import ModelCheckpoint, Callback

# Mengabaikan peringatan
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## Data Preparation

### Data Loading
"""

# Import module yang disediakan google colab untuk kebutuhan upload file
from google.colab import files
files.upload()

# Download kaggle dataset and unzip the file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d muratkokludataset/rice-image-dataset
!unzip rice-image-dataset.zip

"""cek jumlah gambar dan resolusi dataset"""

# Menghitung jumlah gambar masing-masing kelas
def count_image_by_class(dataset_path):
  class_counts ={}
  for class_name in os.listdir(dataset_path):
    class_folder = os.path.join(dataset_path, class_name)
    if os.path.isdir(class_folder):
      class_counts[class_name] = len(os.listdir(class_folder))

  print("\nJumlah gambar per kelas: ")
  for class_name, count in class_counts.items():
    print(f"- {class_name}: {count} gambar")

  return class_counts

# menghitung jumlah resolusi gambar masing-masing kelas
def count_image_by_resolution(dataset_path):
   resolution_counts = defaultdict(int)

   for class_name in os.listdir(dataset_path):
        class_folder = os.path.join(dataset_path, class_name)
        if os.path.isdir(class_folder):
            for image_name in os.listdir(class_folder):
                image_path = os.path.join(class_folder, image_name)
                try:
                    with Image.open(image_path) as img:
                        resolution = img.size
                        resolution_counts[resolution] += 1
                except:
                    print(f" Gagal membaca {image_path}")

   print("\nJumlah gambar berdasarkan resolusi:")
   for resolution, count in resolution_counts.items():
        print(f"- {resolution}: {count} gambar")

   return resolution_counts

dataset_path = "Rice_Image_Dataset"

count_image_by_class(dataset_path)
count_image_by_resolution(dataset_path)

"""----
Ubah resolusi gambar agar beragam
"""

# Ubah resolusi gambar agar beragam
def resize_img(dataset_path, min_resolution=250, max_resolution=300):
  for root, dirs, folder in os.walk(dataset_path):
    for file in folder:
      file_path = os.path.join(root,file)

      if file.lower().endswith((".jpg")):
        with Image.open(file_path) as img:
          new_width = random.randint(min_resolution, max_resolution)
          new_height = random.randint(min_resolution, max_resolution)
          # mengubah ukuran
          resized_images = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

          # menyimpan perubahan resolusi gambar
          resized_images.save(file_path)

dataset_path = "/content/Rice_Image_Dataset"

resize_img(dataset_path)

dataset_path = "Rice_Image_Dataset"

count_image_by_class(dataset_path)
count_image_by_resolution(dataset_path)

# Direktori awal
initial_dir = "Rice_Image_Dataset"

# Direktori baru untuk dataset gabungan
combined_dir = "rice/dataset"

# Buat direktori baru untuk dataset gabungan
os.makedirs(combined_dir, exist_ok=True)

# Salin file dan folder dari direktori awal
for category in os.listdir(initial_dir):
    category_dir = os.path.join(initial_dir, category)
    if os.path.isdir(category_dir):
        shutil.copytree(category_dir, os.path.join(combined_dir, category), dirs_exist_ok=True)

"""Plot gambar sampel untuk semua kelas"""

# Membuat kamus yang menyimpan gambar untuk setiap kelas dalam data
rice_image = {}

# Tentukan path sumber train
path = "rice/"
path_sub = os.path.join(path, "dataset")
for i in os.listdir(path_sub):
    rice_image[i] = os.listdir(os.path.join(path_sub, i))

# Menampilkan secara acak 5 gambar di bawah setiap kelas dari data latih
fig, axs = plt.subplots(len(rice_image.keys()), 5, figsize=(15, 15))

for i, class_name in enumerate(os.listdir(path_sub)):
    images = np.random.choice(rice_image[class_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = os.path.join(path_sub, class_name, image_name)
        img = Image.open(img_path)
        axs[i, j].imshow(img)
        axs[i, j].set(xlabel=class_name, xticks=[], yticks=[])

fig.tight_layout()
plt.show()

"""Plot distribusi gambar di seluruh kelas"""

# Define source path
rice_path = "rice/dataset/"

# Create a list that stores data for each filenames, filepaths, and labels in the data
file_name = []
labels = []
full_path = []

# Get data image filenames, filepaths, labels one by one with looping, and store them as dataframe
for path, subdirs, files in os.walk(rice_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

distribution_train = pd.DataFrame({"path":full_path,'file_name':file_name,"labels":labels})

# Plot the distribution of images across the classes
Label = distribution_train['labels']
plt.figure(figsize = (6,6))
sns.set_style("darkgrid")
plot_data = sns.countplot(Label)

"""### Data Preprocessing

#### Split Dataset
"""

# Panggil variabel mypath yang menampung folder dataset gambar
mypath= 'rice/dataset/'

file_name = []
labels = []
full_path = []
for path, subdirs, files in os.walk(mypath):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)


# Memasukan variabel yang sudah dikumpulkan pada looping di atas menjadi sebuah dataframe agar rapih
df = pd.DataFrame({"path":full_path,'file_name':file_name,"labels":labels})
# Melihat jumlah data gambar pada masing-masing label
df.groupby(['labels']).size()

"""Bagi dataset menjadi 70% train, 15% validation, dan 15% test"""

# Variabel yang digunakan pada pemisahan data ini dimana variabel x = data path dan y = data labels
X = df['path']
y = df['labels']

# Split 15% test dari seluruh data
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.15, random_state=300)

# Split 17.6% validation dari 85% data yang tersisa (â‰ˆ 15% dari total)
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.176, random_state=300)

# Menyatukan kedalam masing-masing dataframe
df_tr = pd.DataFrame({'path':X_train,'labels':y_train,'set':'train'})
df_val = pd.DataFrame({'path':X_val,'labels':y_val,'set':'validation'})
df_te = pd.DataFrame({'path':X_test,'labels':y_test,'set':'test'})

# Print hasil diatas untuk melihat panjang size data training dan testing
print('train size', len(df_tr))
print('validation size', len(df_val))
print('test size', len(df_te))

# Gabungkan DataFrame df_tr, df_te dan df_val
df_all = pd.concat([df_tr, df_te, df_val], ignore_index=True)

print('===================================================== \n')
print(df_all.groupby(['set', 'labels']).size(), '\n')
print('===================================================== \n')

# Cek sample data
print(df_all.sample(5))

# Memanggil dataset asli yang berisi keseluruhan data gambar yang sesuai dengan labelnya
datasource_path = "rice/dataset/"
# Membuat variabel Dataset, dimana nanti menampung data yang telah dilakukan pembagian data training dan testing
dataset_path = "Dataset-Final/"

# Menyalin file gambar dari lokasi sumber ke direktori tujuan yang terstruktur berdasarkan set data
for index, row in tq(df_all.iterrows()):
    # Deteksi filepath
    file_path = row['path']
    if os.path.exists(file_path) == False:
            file_path = os.path.join(datasource_path,row['labels'],row['image'].split('.')[0])

    # Buat direktori tujuan folder
    if os.path.exists(os.path.join(dataset_path,row['set'],row['labels'])) == False:
        os.makedirs(os.path.join(dataset_path,row['set'],row['labels']))

    # Tentukan tujuan file
    destination_file_name = file_path.split('/')[-1]
    file_dest = os.path.join(dataset_path,row['set'],row['labels'],destination_file_name)

    # Salin file dari sumber ke tujuan
    if os.path.exists(file_dest) == False:
        shutil.copy2(file_path,file_dest)

""" Image Data Generator

"""

# Define training and test directories
TRAIN_DIR = "Dataset-Final/train/"
TEST_DIR = "Dataset-Final/test/"
VAL_DIR = "Dataset-Final/validation/"

# Daftar label
labels = [
    'Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag'
]

# Cetak jumlah gambar di setiap set
for label in labels:
    train_path = os.path.join(TRAIN_DIR, label)
    test_path = os.path.join(TEST_DIR, label)
    val_path = os.path.join(VAL_DIR, label)

    print(f"Train - {label}: {len(os.listdir(train_path))}")
    print(f"Test  - {label}: {len(os.listdir(test_path))}")
    print(f"Val   - {label}: {len(os.listdir(val_path))}")

# Create an ImageDataGenerator object normalizing the images
datagen = ImageDataGenerator(rescale=1/255.)
val_datagen = ImageDataGenerator(rescale=1/255.)
test_datagen = ImageDataGenerator(rescale=1. / 255)

train_generator = datagen.flow_from_directory(TRAIN_DIR,
                                              batch_size=32,
                                              target_size=(150,150),
                                              color_mode="rgb",
                                              class_mode='categorical',
                                              subset='training',
                                              shuffle=True)

validation_generator = val_datagen.flow_from_directory(VAL_DIR,
                                                   batch_size=32,
                                                   target_size=(150,150),
                                                   color_mode="rgb",
                                                   class_mode='categorical',
                                                   shuffle=False)

test_generator = test_datagen.flow_from_directory(TEST_DIR,
                                                  batch_size=1,
                                                  target_size=(150,150),
                                                  color_mode="rgb",
                                                  class_mode='categorical',
                                                  shuffle=False)

"""Parameter di dalam `datagen.flow_from_directory`:

* **`TRAIN_DIR`**: Ini adalah **lokasi folder utama** tempat gambar-gambar pelatihan Anda berada. Keras akan mencari subfolder di dalam `TRAIN_DIR` ini, dan setiap subfolder akan dianggap sebagai satu kelas. Misalnya, jika `TRAIN_DIR` adalah `/data/gambar_beras/train` dan di dalamnya ada folder `beras_putih` dan `beras_merah`, Keras akan otomatis mengenali dua kelas tersebut.

* **`batch_size=32`**: Parameter ini menentukan **berapa banyak gambar yang akan dimuat dan diproses sekaligus** dalam satu *batch* (kelompok) saat pelatihan model. Menggunakan *batch* membantu mengelola memori dan mempercepat proses pelatihan. Jadi, setiap kali model mengambil data untuk belajar, ia akan mendapatkan 32 gambar.

* **`target_size=(150,150)`**: Ini adalah **ukuran (resolusi) yang diinginkan untuk semua gambar** yang dimuat. Meskipun gambar asli mungkin memiliki ukuran yang berbeda-beda, `flow_from_directory` akan mengubah ukurannya menjadi 150x150 piksel sebelum memberikannya ke model. Ini penting agar semua *input* model memiliki dimensi yang konsisten.

* **`color_mode="rgb"`**: Parameter ini menetapkan **mode warna gambar**. `"rgb"` berarti gambar akan dimuat sebagai gambar berwarna dengan tiga saluran (merah, hijau, biru). Pilihan lain bisa `"grayscale"` untuk gambar hitam putih.

* **`class_mode='categorical'`**: Ini menentukan **bagaimana label kelas akan dikodekan**. `'categorical'` digunakan ketika Anda memiliki lebih dari dua kelas dan setiap kelas direpresentasikan sebagai vektor *one-hot encoded* (misalnya, `[1, 0, 0]` untuk kelas pertama, `[0, 1, 0]` untuk kelas kedua, dst.). Ini cocok untuk masalah klasifikasi multi-kelas.

* **`subset='training'`**: Parameter ini relevan jika Anda membagi dataset Anda menjadi subset pelatihan dan validasi menggunakan `validation_split` di `ImageDataGenerator`. Dalam kasus ini, `subset='training'` secara eksplisit memberitahu generator untuk **hanya mengambil data yang dialokasikan untuk set pelatihan**.

* **`shuffle=True`**: Ini berarti **gambar-gambar akan diacak (di-shuffle) urutannya** sebelum setiap *epoch* pelatihan. Mengacak data sangat penting untuk mencegah model mempelajari urutan data dan memastikan pelatihan yang lebih robust.

## Modelling
"""

# tf.keras.backend.clear_session()

####################### Init sequential model ##################################
model = Sequential()

# ######################### Input layer with Fully Connected Layer ################################
# 1st Convolutional layer, Batch Normalization layer, and Pooling layer
model.add(Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(150,150,3)))
model.add(MaxPooling2D((2, 2)))

# 2nd Convolutional layer, Batch Normalization layer, and Pooling layer
model.add(Conv2D(16, (3, 3),padding='same', activation='relu'))
model.add(MaxPooling2D((2, 2)))

# Flatten layer
model.add(Flatten())
# 1nd Dense Layer
model.add(Dense(32, activation = 'relu'))
# 1nd Dropout Layer
model.add(Dropout(0.3))
# 2nd Dense Layer
model.add(Dense(5, activation = 'softmax'))
######################### Fully Connected Layer ################################

######################### Compile Model ################################
model.compile(optimizer=tf.keras.optimizers.Adam(),
                loss='categorical_crossentropy',
                metrics=['accuracy'])

# Summary of the Model Architecture
print(model.summary())

"""Model ini adalah jaringan saraf konvolusi (CNN) sekuensial yang dirancang untuk klasifikasi gambar.

Fungsi dari setiap jenis *layer* yang digunakan dalam model CNN tersebut:

### Penjelasan Mendasar Tiap Layer:

1.  **`Conv2D` (Convolutional Layer / Layer Konvolusi 2D)**:
    * **Apa itu?** Ini adalah "otak" utama dari CNN. Bayangkan seperti **detektor fitur** yang bergerak di seluruh gambar.
    * **Fungsi:** Ia belajar untuk **mengidentifikasi pola-pola lokal** dalam gambar, seperti tepi, sudut, tekstur, atau bentuk dasar. Ini dilakukan dengan menggunakan "filter" atau "kernel" kecil yang digeser di atas gambar, melakukan operasi matematika (konvolusi), dan menghasilkan "feature map" yang menyoroti di mana pola-pola tersebut ditemukan.

2.  **`MaxPooling2D` (Max Pooling Layer / Layer Max Pooling 2D)**:
    * **Apa itu?** Ini adalah **pengecil ukuran gambar atau "penyaring detail"**.
    * **Fungsi:** Setelah `Conv2D` mendeteksi fitur, `MaxPooling2D` **mengurangi ukuran *feature map*** yang dihasilkan. Ia melakukannya dengan mengambil nilai piksel terbesar (maksimum) dari area kecil (misalnya, 2x2 piksel) dan membuang sisanya. Ini membantu **mengurangi jumlah data yang harus diproses**, membuat model lebih cepat, dan yang terpenting, membuatnya **lebih tahan terhadap sedikit pergeseran atau distorsi** pada gambar (*spatial invariance*).

3.  **`Flatten` (Flatten Layer)**:
    * **Apa itu?** Ini adalah **"perata" atau "pelurus" data**.
    * **Fungsi:** Setelah serangkaian `Conv2D` dan `MaxPooling2D` yang menghasilkan data multidimensi (seperti tabel 2D atau 3D), `Flatten` akan **mengubah semua data tersebut menjadi satu baris panjang (vektor 1D)**. Ini diperlukan karena *layer* berikutnya (`Dense`) hanya bisa memproses *input* dalam bentuk satu dimensi.

4.  **`Dense` (Fully Connected Layer / Layer Terhubung Penuh)**:
    * **Apa itu?** Ini adalah **"pengambil keputusan" atau "pengklasifikasi"** di akhir model.
    * **Fungsi:** Setiap *neuron* di *layer* ini **terhubung ke setiap *neuron* dari *layer* sebelumnya**. Berdasarkan fitur-fitur yang telah diekstraksi oleh *layer* `Conv2D` dan `Pooling`, *layer Dense* ini akan melakukan perhitungan kompleks untuk **mengidentifikasi dan mengklasifikasikan pola-pola tingkat tinggi** yang menunjukkan kelas mana yang paling mungkin untuk gambar *input*.

5.  **`Dropout` (Dropout Layer)**:
    * **Apa itu?** Ini adalah **"pencegah lupa" atau "pencegah *overfitting*"**.
    * **Fungsi:** Selama pelatihan, `Dropout` secara acak **mematikan (mengabaikan) sebagian *neuron*** (dan koneksinya) untuk sementara waktu. Ini seperti memaksa jaringan untuk tidak terlalu bergantung pada *neuron* tertentu, sehingga model menjadi **lebih robust dan belajar fitur yang lebih umum**, bukan hanya menghafal data pelatihan. Ini sangat efektif untuk mengurangi *overfitting*.

Singkatnya, **`Conv2D` mencari fitur**, **`MaxPooling2D` mengecilkan dan menyaring**, **`Flatten` meratakan data**, **`Dense` membuat keputusan klasifikasi**, dan **`Dropout` mencegah model terlalu menghafal**.

----

### Penjelasan Per Layer:

* **`tf.keras.backend.clear_session()`**: Mengatur ulang sesi Keras untuk memastikan model dibangun dari awal.
* **`Sequential()`**: Memulai model sebagai urutan *layer* yang linear.
* **`Conv2D(16, (3, 3), padding='same', activation='relu', input_shape=(150,150,3))`**: **Layer konvolusi pertama** dengan 16 *filter* 3x3, menjaga ukuran *output* sama, menggunakan aktivasi ReLU, dan menerima gambar RGB 150x150.
* **`MaxPooling2D((2, 2))`**: **Layer *max pooling* pertama** yang mengurangi dimensi spasial gambar (misalnya, dari 150x150 menjadi 75x75) untuk mengurangi komputasi dan parameter.
* **`Conv2D(16, (3, 3), padding='same', activation='relu')`**: **Layer konvolusi kedua** yang juga menggunakan 16 *filter* 3x3 dengan aktivasi ReLU, belajar fitur yang lebih kompleks.
* **`MaxPooling2D((2, 2))`**: **Layer *max pooling* kedua** untuk reduksi dimensi spasial lebih lanjut.
* **`Flatten()`**: Mengubah *output* dari *layer* konvolusi dan *pooling* (yang berbentuk 2D/3D) menjadi **vektor 1D tunggal** agar bisa diproses oleh *layer Dense*.
* **`Dense(32, activation = 'relu')`**: **Layer *Fully Connected* pertama** dengan 32 *neuron* dan aktivasi ReLU.
* **`Dropout(0.3)`**: **Layer *dropout*** yang secara acak menonaktifkan 30% *neuron* selama pelatihan untuk mencegah *overfitting*.
* **`Dense(5, activation = 'softmax')`**: **Layer *output* akhir** dengan 5 *neuron* (sesuai jumlah kelas) dan aktivasi *softmax* untuk menghasilkan probabilitas klasifikasi per kelas.

---

### Kompilasi Model:

* **`optimizer=tf.keras.optimizers.Adam()`**: Menggunakan **algoritma Adam** untuk mengoptimalkan *weight* model.
* **`loss='categorical_crossentropy'`**: Fungsi *loss* yang cocok untuk **klasifikasi multi-kelas** dengan label *one-hot encoded*.
* **`metrics=['accuracy']`**: Metrik yang digunakan untuk **mengukur kinerja model** selama pelatihan dan evaluasi (yaitu, akurasi).

---

### Ringkasan Model:

* **`print(model.summary())`**: Menampilkan **ringkasan arsitektur model**, termasuk bentuk *output* setiap *layer* dan jumlah total parameter yang bisa dilatih.
"""

# Commented out IPython magic to ensure Python compatibility.
# Daftar label (pastikan urutannya sama dengan urutan class_indices di generator)
labels = ['Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag']

# Ambil class indices dari train_generator
class_indices = train_generator.class_indices  # misalnya: {'daisy': 0, 'dandelion': 1, ...}

# Ambil label dari data generator (integer label)
y_train = train_generator.classes

# Hitung class weights
class_weights_array = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)

# Konversi ke dictionary
class_weights = dict(enumerate(class_weights_array))

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.98):
      print("\nAccuracy has been achieved >98%")
      self.model.stop_training = True

callbacks = myCallback()

# %time

# Fitting / training model
history = model.fit(train_generator,
                        epochs=16,
                        validation_data=validation_generator,
                        verbose=1,
                        class_weight = class_weights,
                        callbacks=[callbacks])

"""### Kompilasi Model: Penjelasan Singkat

`model.compile()` adalah langkah penting untuk **mengatur cara model Anda akan belajar**.

* **`optimizer=tf.keras.optimizers.Adam()`**: Ini adalah **algoritma "belajar"** yang akan menyesuaikan bobot model untuk mengurangi kesalahan. **Adam** adalah pilihan populer karena efisien.
* **`loss='categorical_crossentropy'`**: Ini adalah **"pengukur kesalahan"** model. Ia menghitung seberapa jauh prediksi model dari kenyataan, dan tujuannya adalah untuk meminimalkan nilai ini. Cocok untuk klasifikasi multi-kelas dengan label *one-hot encoded*.
* **`metrics=['accuracy']`**: Ini adalah **"indikator kinerja"** yang akan ditampilkan selama pelatihan, seperti akurasi, untuk melihat seberapa baik model memprediksi dengan benar.

Singkatnya, `model.compile()` memberitahu model **bagaimana cara belajar (optimizer), apa yang harus diminimalkan (loss), dan bagaimana mengukur keberhasilannya (metrics)**.

## Evaluasi dan Visualisasi
"""

# Cek akurasi
train_acc = history.history['accuracy'][-1]
val_acc = history.history['val_accuracy'][-1]
test_loss, test_acc = model.evaluate(test_generator, verbose=1)

print(f"Training Accuracy: {train_acc:.2%}")
print(f"Validation Accuracy: {val_acc:.2%}")
print(f"Test Accuracy: {test_acc:.2%}")

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r')
plt.plot(epochs, val_acc, 'b')
plt.title('Training and Validation Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(epochs, loss, 'r')
plt.plot(epochs, val_loss, 'b')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.title('Training and Validaion Loss')
plt.show()

# Reset generator
test_generator.reset()

# Prediksi (probabilitas softmax)
preds = model.predict(test_generator, verbose=0)

# Ambil label prediksi (kelas dengan probabilitas tertinggi)
pred_classes = np.argmax(preds, axis=1)

# Ambil label sebenarnya dari test generator
true_classes = test_generator.classes

# Ambil label string dari kelas (misalnya: ['Arborio', 'Basmati', ...])
class_labels = list(test_generator.class_indices.keys())

# Confusion Matrix
cm = confusion_matrix(true_classes, pred_classes)

# Visualisasi Confusion Matrix
plt.figure(figsize=(8,6))
sns.heatmap(pd.DataFrame(cm, index=class_labels, columns=class_labels), annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Classification Report
print("\nClassification Report:\n")
print(classification_report(true_classes, pred_classes, target_names=class_labels, digits=4))

"""## Konversi Model"""

# Save the model as a .h5 file
model.save('rice_model.h5')

# Convert and save the model to TensorFlow Lite format
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

with open('rice_model.tflite', 'wb') as f:
  f.write(tflite_model)

# Save class labels to a text file for TensorFlow Lite
# Define the actual class labels
class_labels = ['Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag']

with open('labels.txt', 'w') as f:
  for label in class_labels:  # Iterate over the class labels
      f.write(label + '\n')

# Convert and save the model to TensorFlow.js format
import tensorflowjs as tfjs

tfjs.converters.save_keras_model(model, 'rice_model_tfjs')

# Save the model as a .pb file (SavedModel format)
model.export('rice_model_pb')

"""## Inference (Optional)"""

# === Muat model TFLite ===
interpreter = tf.lite.Interpreter(model_path="tflite/model.tflite")
interpreter.allocate_tensors()

# Ambil informasi tensor input dan output
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# === Ambil satu gambar contoh untuk inferensi ===
img_path = 'rice/dataset/Jasmine/Jasmine (10).jpg'
img = Image.open(img_path).resize((150, 150))  # Resize sesuai input model
img_array = np.array(img) / 255.0  # Normalisasi
if len(img_array.shape) == 2:  # Jika grayscale, ubah ke 3 channel
    img_array = np.stack((img_array,) * 3, axis=-1)

input_data = np.expand_dims(img_array, axis=0).astype(np.float32)

# === Inference ===
interpreter.set_tensor(input_details[0]['index'], input_data)
interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
predicted_class = np.argmax(output_data)
confidence = np.max(output_data)

# Label kelas (sesuai urutan training)
labels = ['Arborio', 'Basmati', 'Ipsala', 'Jasmine', 'Karacadag']

# === Tampilkan hasil ===
plt.imshow(img)
plt.axis('off')
plt.title(f"Predicted: {labels[predicted_class]} ({confidence:.2f})")
plt.show()